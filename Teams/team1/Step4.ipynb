{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d63e00a-825f-449e-862b-9844ffe5e972",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f00f241-9a7d-4669-8856-c1ac584e51c2",
   "metadata": {},
   "source": [
    "# ReadME\n",
    "## Experiment Analysis\n",
    "\n",
    "### Overview\n",
    "This dataset contains the results of 3410 runs from 22 separate experiments designed to benchmark data‑processing operations. Each run reports CPU time, memory usage, throughput and other performance metrics for a batch of 512 samples processed on a CPU. The experiments use different input files ranging from 10 MB to 1 GB, and the number of batches varies per run, which explains the variation in time and memory consumption.\n",
    "\n",
    "Variables\n",
    "The following variables are recorded for each run:\n",
    "\n",
    "`batch_size` – number of samples processed in each batch.\n",
    "\n",
    "`data_path` – relative path to the input data file for the run.\n",
    "\n",
    "`device` – hardware used for the run (all experiments used the CPU).\n",
    "\n",
    "`execution_time` (seconds/batch) – time taken to process one batch.\n",
    "\n",
    "`library_overhead_memory` (MB) – memory overhead of the processing library.\n",
    "\n",
    "`num_batches` – number of batches processed.\n",
    "\n",
    "`rank` – optional ranking of certain runs.\n",
    "\n",
    "`result_path` – directory where output data were written.\n",
    "\n",
    "`sample_persec` – number of samples processed per second.\n",
    "\n",
    "`throughput_bps` – data throughput in bytes per second.\n",
    "\n",
    "`total_cpu_memory` (MB) – total CPU memory usage.\n",
    "\n",
    "`total_cpu_time` (seconds) – total CPU time consumed by the run.\n",
    "\n",
    "`total_image_memory` (MB) – memory used to store images.\n",
    "\n",
    "`total_model_memory` (MB) – memory used by the model.\n",
    "\n",
    "`total_process_memory` (MB) – total memory consumed by the process.\n",
    "\n",
    "**Important variables**\n",
    "Among the available variables, some provide a concise picture of performance:\n",
    "\n",
    "*total CPU time (seconds)* – the overall time spent on CPU; lower values mean better efficiency.\n",
    "\n",
    "*total CPU memory (MB)* – peak memory footprint during the run.\n",
    "\n",
    "*execution time per batch (s)* – how long one batch takes on average.\n",
    "\n",
    "*throughput (MB/s)* – amount of data processed per second; higher throughput indicates better performance.\n",
    "\n",
    "*samples per second* – how many samples were processed per second, independent of data size.\n",
    "\n",
    "These metrics were used to compare the experiments and create visualisations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb338ba-c3b6-475c-a02b-20f26dc136f1",
   "metadata": {},
   "source": [
    "### Summary table  \n",
    "Experiments are sorted in ascending order by their index to make comparisons straightforward. The table below summarises the average values of the important metrics for each experiment. Two additional columns are included:\n",
    "\n",
    "Data Prefix – the size of the input file derived from the data_path (e.g., 10MB, 25MB, 100MB).\n",
    "\n",
    "File Limit – a cyclic label (2, 4, 6, 8, 10) assigned sequentially across experiments to group runs into five categories.\n",
    "\n",
    "Throughput values are converted to megabytes per second for readability. Averages are computed across all runs belonging to each experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d467c1c-1ce5-4df0-b305-df48e6159ba3",
   "metadata": {},
   "source": [
    "| Experiment   | Data Prefix | File Limit | Avg CPU Time (s) | Avg CPU Memory (MB) | Avg Exec Time per Batch (s) | Avg Throughput (MB/s) | Avg Samples/s |\n",
    "| ------------ | ----------- | ---------- | ---------------- | ------------------- | --------------------------- | --------------------- | ------------- |\n",
    "| Experiment1  | 100MB       | 2          | 25.34            | 14295.35            | 0.33                        | 5.09                  | 201.66        |\n",
    "| Experiment2  | 10MB        | 4          | 3.56             | 7151.39             | 3.57                        | 23.70                 | 144.36        |\n",
    "| Experiment3  | 10MB        | 2          | 3.56             | 7151.39             | 3.57                        | 23.70                 | 144.36        |\n",
    "| Experiment4  | 10MB        | 4          | 3.56             | 7151.39             | 3.57                        | 23.70                 | 144.36        |\n",
    "| Experiment5  | 10MB        | 6          | 3.56             | 7151.39             | 3.57                        | 23.70                 | 144.36        |\n",
    "| Experiment6  | 10MB        | 8          | 3.56             | 7151.39             | 3.57                        | 23.70                 | 144.36        |\n",
    "| Experiment7  | 25MB        | 10         | 7.28             | 17864.66            | 2.92                        | 31.04                 | 189.07        |\n",
    "| Experiment8  | 25MB        | 2          | 7.28             | 17864.66            | 2.92                        | 31.04                 | 189.07        |\n",
    "| Experiment9  | 25MB        | 4          | 7.28             | 17864.66            | 2.92                        | 31.04                 | 189.07        |\n",
    "| Experiment10 | 25MB        | 6          | 7.28             | 17864.66            | 2.92                        | 31.04                 | 189.07        |\n",
    "| Experiment11 | 50MB        | 8          | 11.89            | 35579.80            | 2.40                        | 37.05                 | 225.67        |\n",
    "| Experiment15 | 50MB        | 10         | 11.89            | 35579.80            | 2.40                        | 37.05                 | 225.67        |\n",
    "| Experiment16 | 75MB        | 2          | 18.37            | 53229.60            | 2.47                        | 37.21                 | 226.64        |\n",
    "| Experiment17 | 75MB        | 4          | 18.37            | 53229.60            | 2.47                        | 37.21                 | 226.64        |\n",
    "| Experiment18 | 75MB        | 6          | 18.37            | 53229.60            | 2.47                        | 37.21                 | 226.64        |\n",
    "| Experiment19 | 75MB        | 8          | 18.37            | 53229.60            | 2.47                        | 37.21                 | 226.64        |\n",
    "| Experiment20 | 75MB        | 10         | 18.37            | 53229.60            | 2.47                        | 37.21                 | 226.64        |\n",
    "| Experiment21 | 100MB       | 2          | 22.35            | 70800.08            | 2.28                        | 38.96                 | 237.35        |\n",
    "| Experiment22 | 100MB       | 4          | 22.35            | 70800.08            | 2.28                        | 38.96                 | 237.35        |\n",
    "| Experiment23 | 100MB       | 6          | 22.35            | 70800.08            | 2.28                        | 38.96                 | 237.35        |\n",
    "| Experiment24 | 100MB       | 8          | 22.35            | 70800.08            | 2.28                        | 38.96                 | 237.35        |\n",
    "| Experiment25 | 100MB       | 10         | 22.35            | 70800.08            | 2.28                        | 38.96                 | 237.35        |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88af5e1b-5d20-4e12-be5a-6c14b80c1830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the experiment results (CSV created from the raw `.txt` file)\n",
    "df = pd.read_csv('experiments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89e6602b-7065-4efb-a2cc-a319064f4a9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['experiment', 'file_limit', 'data_prefix', 'avg_total_cpu_time_seconds',\n",
       "       'avg_execution_time_seconds_batch', 'avg_throughput_bps',\n",
       "       'avg_sample_persec', 'n_records'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "231bdeb2-9f97-429d-a60f-c1fb72414250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure numeric columns are numeric\n",
    "numeric_cols = [\n",
    "    'avg_total_cpu_time_seconds',\n",
    "    'avg_execution_time_seconds_batch',\n",
    "    'avg_throughput_bps',\n",
    "    'avg_sample_persec',\n",
    "    'n_records'\n",
    "]\n",
    "for c in numeric_cols:\n",
    "    df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "\n",
    "# Convenience metric (MB/s) & optional CSV\n",
    "df['avg_throughput_MBps'] = df['avg_throughput_bps'] / 1e6\n",
    "df.to_csv('experiment_summary.csv', index=False)          # updated CSV\n",
    "\n",
    "# Plot settings\n",
    "sns.set_theme(style='whitegrid')\n",
    "\n",
    "## Horizontal bar: throughput (MB/s) \n",
    "sorted_df = df.sort_values('avg_throughput_MBps')\n",
    "\n",
    "plt.figure(figsize=(8, 7))\n",
    "\n",
    "# Use default palette (no warning) or set hue='experiment'\n",
    "sns.barplot(\n",
    "    data=sorted_df,\n",
    "    y='experiment',\n",
    "    x='avg_throughput_MBps'\n",
    ")\n",
    "plt.xlabel('Average Throughput (MB/s)')\n",
    "plt.ylabel('Experiment')\n",
    "plt.title('Average Throughput per Experiment (MB/s)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('avg_throughput_MBps.png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "## Scatter: execution time vs. samples/s (with a legend) \n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = sns.scatterplot(\n",
    "    data=df,\n",
    "    x='avg_execution_time_seconds_batch',\n",
    "    y='avg_sample_persec',\n",
    "    hue='experiment',        # colour by experiment\n",
    "    palette='tab10',\n",
    "    s=100,\n",
    "    legend='brief'           # show legend\n",
    ")\n",
    "plt.xlabel('Average Execution Time per Batch (s)')\n",
    "plt.ylabel('Average Samples per Second')\n",
    "plt.title('Execution Time vs Samples per Second per Experiment')\n",
    "\n",
    "## Annotate each point\n",
    "for _, row in df.iterrows():\n",
    "    scatter.annotate(\n",
    "        row['experiment'],\n",
    "        (row['avg_execution_time_seconds_batch'], row['avg_sample_persec']),\n",
    "        textcoords=\"offset points\",\n",
    "        xytext=(5, -5),\n",
    "        ha='left',\n",
    "        fontsize=7\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('execution_vs_sample_persec_refined.png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "##Line: total CPU time across experiments \n",
    "df['experiment'] = df['experiment'].astype(str)          # ensure string dtype\n",
    "df['exp_num'] = df['experiment'].str.extract(r'(\\d+)').astype(int)\n",
    "\n",
    "df_sorted = df.sort_values('exp_num')\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.lineplot(\n",
    "    data=df_sorted,\n",
    "    x='exp_num',\n",
    "    y='avg_total_cpu_time_seconds',\n",
    "    marker='o',\n",
    "    linewidth=2\n",
    ")\n",
    "plt.xticks(df_sorted['exp_num'], df_sorted['experiment'], rotation=45, ha='right')\n",
    "plt.xlabel('Experiment')\n",
    "plt.ylabel('Average Total CPU Time (s)')\n",
    "plt.title('Average Total CPU Time per Experiment')\n",
    "plt.tight_layout()\n",
    "plt.savefig('avg_cpu_time_refined.png', dpi=300)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64e9ab6-8d33-4a40-8f54-962ac88ade13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b82f65e-b264-43c2-96f7-a099a1fbf5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partition  Requests  Duration_s  Memory_GB  Cost ($)\n",
      "     10MB       100        3.56        7.0       NaN\n",
      "     25MB      1600        7.28       17.4      0.16\n",
      "     50MB       410       11.89       34.7      0.20\n",
      "     75MB       685       18.37       52.0      0.30\n",
      "    100MB       615       22.84       60.2      0.38\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Load & parse All Experiments.txt\n",
    "txt_path = Path(\"All Experiments.txt\")\n",
    "\n",
    "# Read the whole file\n",
    "raw = txt_path.read_text()\n",
    "\n",
    "# Regex to capture each \"ExperimentX\" section followed by its JSON list\n",
    "pattern = re.compile(r\"Experiment\\d+\\s*\\n(\\[.*?\\])\", re.S)\n",
    "all_runs = []\n",
    "\n",
    "for match in pattern.finditer(raw):\n",
    "    # clean up JSON (remove stray trailing commas if any)\n",
    "    json_blob = match.group(1)\n",
    "    runs = json.loads(json_blob)\n",
    "    all_runs.extend(runs)\n",
    "\n",
    "df = pd.DataFrame(all_runs)\n",
    "\n",
    "# Clean up / ensure numeric columns\n",
    "numeric_cols = [\n",
    "    \"total_cpu_time (seconds)\",\n",
    "    \"total_cpu_memory (MB)\",\n",
    "    \"execution_time (seconds/batch)\",\n",
    "    \"throughput_bps\",\n",
    "    \"sample_persec\",\n",
    "    \"num_batches\",\n",
    "]\n",
    "for col in numeric_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "# Derive helper columns\n",
    "\n",
    "# Partition size (data_prefix) from data_path, e.g. \"25MB\"\n",
    "df[\"partition\"] = df[\"data_path\"].str.split(\"/\").str[0]\n",
    "\n",
    "# Duration (seconds) – use total CPU time (method A)\n",
    "df[\"duration_s\"] = df[\"total_cpu_time (seconds)\"]\n",
    "\n",
    "# Aggregate per partition\n",
    "summary = (\n",
    "    df.groupby(\"partition\")\n",
    "      .agg(\n",
    "          Requests=(\"partition\", \"size\"),\n",
    "          Duration_s=(\"duration_s\", \"mean\"),\n",
    "          Memory_GB=(\"total_cpu_memory (MB)\", lambda x: x.mean() / 1024)\n",
    "      )\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "# Add Cost ($) via fixed lookup\n",
    "\n",
    "partition_cost_lookup = {\n",
    "    \"25MB\": 0.16,\n",
    "    \"50MB\": 0.20,\n",
    "    \"75MB\": 0.30,\n",
    "    \"100MB\": 0.38\n",
    "}\n",
    "summary[\"Cost ($)\"] = summary[\"partition\"].map(partition_cost_lookup)\n",
    "\n",
    "# Tidy formatting\n",
    "\n",
    "summary[\"Duration_s\"] = summary[\"Duration_s\"].round(2)\n",
    "summary[\"Memory_GB\"]  = summary[\"Memory_GB\"].round(1)\n",
    "\n",
    "# Sort partitions numerically (25, 50, 75, 100)\n",
    "summary[\"part_num\"] = summary[\"partition\"].str.extract(r\"(\\d+)\").astype(int)\n",
    "summary = summary.sort_values(\"part_num\").drop(columns=\"part_num\")\n",
    "\n",
    "\n",
    "# Create CSV\n",
    "summary.to_csv(\"partition_cost_table.csv\", index=False)\n",
    "print(summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6eae891-edb3-4239-886e-8d085e3dd9ab",
   "metadata": {},
   "source": [
    "### Failed 240 batch job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6411a18-1aa7-4583-bfd6-ad8b650968c4",
   "metadata": {},
   "source": [
    "The workflow failed because the Map state tried to extract `$.body`, but your init Lambda was already returning a bare array, so the iterator handed each Lambda a list instead of a single job object and the code couldn’t find the `\"bucket\"` key. When you switched `Payload.$`, you entered an invalid value—only `$`, a valid JSONPath, or the special context path `$$Map.Item.Value` is allowed—so the definition validator rejected it. Wrap the job list in an object (or point `ItemsPath` to `$`) and set `Payload.$` to `$$Map.Item.Value` so each iteration receives one job record containing `\"bucket\"`, fixing both the runtime KeyError and the save‑time validation error."
   ]
  },
  {
   "cell_type": "raw",
   "id": "0a046869-168b-4c41-8964-84985431526f",
   "metadata": {},
   "source": [
    "{\n",
    "  \"errorMessage\": \"'bucket'\",\n",
    "  \"errorType\": \"KeyError\",\n",
    "  \"requestId\": \"72c88769-f571-4e5e-933d-c97e206bad9e\",\n",
    "  \"stackTrace\": [\n",
    "    \"  File \\\"/var/task/lambda_function.py\\\", line 35, in lambda_handler\\n    bucket = event[\\\"bucket\\\"]\\n\"\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fb4062-af1e-4f14-acc6-509c08e96c88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
