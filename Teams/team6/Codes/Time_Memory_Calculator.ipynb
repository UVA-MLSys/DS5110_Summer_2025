{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9f0cccb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                filename  avg_cpu_time  avg_process_memory  \\\n",
      "0   100MB-1file-128.json     25.759324         2138.394531   \n",
      "1   100MB-1file-256.json     20.604544         4361.402344   \n",
      "2    100MB-1File-32.json    225.481818         2738.230469   \n",
      "3   100MB-1file-512.json     21.341026         4517.609375   \n",
      "4    100MB-1file-64.json     22.514245         1681.699219   \n",
      "5     100MB-1GB-256.json     24.066571         4313.574219   \n",
      "6      10MB-1GB-256.json      3.328900         1186.802790   \n",
      "7      10MB-2GB-256.json      3.092958         1190.260110   \n",
      "8      10MB-4GB-256.json      2.751213         1180.918400   \n",
      "9       10MB-4GB-64.json      4.244186          743.794338   \n",
      "10     10MB-6GB-256.json      2.799368         1186.847075   \n",
      "11     10MB-8GB-256.json      3.601294         1187.793519   \n",
      "12    25MB-10GB-256.json      6.459697         1737.696618   \n",
      "13    25MB-12GB-256.json      6.391909         1730.849077   \n",
      "14     25MB-1GB-256.json      6.465970         1720.837426   \n",
      "15     25MB-2GB-256.json      5.994699         1740.108470   \n",
      "16     25MB-4GB-256.json      6.599597         1739.011552   \n",
      "17     25MB-6GB-256.json      6.376500         1724.735915   \n",
      "18     25MB-8GB-256.json      6.296281         1727.573373   \n",
      "19     50MB-1GB-256.json     11.523692         2564.420945   \n",
      "20     75MB-1GB-256.json     18.530627         3434.301339   \n",
      "\n",
      "    avg_throughput_bps  record_count  \n",
      "0         3.258432e+07             1  \n",
      "1         4.073616e+07             1  \n",
      "2         3.722473e+06             1  \n",
      "3         3.933035e+07             1  \n",
      "4         3.728084e+07             1  \n",
      "5         3.535165e+07            11  \n",
      "6         2.607668e+07           105  \n",
      "7         2.886155e+07           204  \n",
      "8         3.301102e+07           408  \n",
      "9         2.550153e+07           408  \n",
      "10        3.219871e+07           612  \n",
      "11        2.493032e+07           816  \n",
      "12        3.461241e+07           410  \n",
      "13        3.496325e+07           492  \n",
      "14        3.296535e+07            42  \n",
      "15        3.654777e+07            82  \n",
      "16        3.374877e+07           164  \n",
      "17        3.466726e+07           246  \n",
      "18        3.526903e+07           328  \n",
      "19        3.699282e+07            21  \n",
      "20        3.475404e+07            14  \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# Get the current directory where this script is located\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Find all JSON files in the current directory\n",
    "json_files = glob.glob(os.path.join(current_dir, \"*.json\"))\n",
    "\n",
    "# List to collect summary stats for each file\n",
    "summary_data = []\n",
    "\n",
    "# Loop through each JSON file\n",
    "for file_path in json_files:\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = json.load(f)  # List of objects\n",
    "\n",
    "        # Initialize accumulators for this file\n",
    "        total_cpu_time_sum = 0\n",
    "        total_process_memory_sum = 0\n",
    "        execution_time_sum = 0\n",
    "        throughput_bps_sum = 0\n",
    "        count = len(data)\n",
    "\n",
    "        for obj in data:\n",
    "            total_cpu_time_sum += obj.get(\"total_cpu_time (seconds)\", 0)\n",
    "            total_process_memory_sum += obj.get(\"total_process_memory (MB)\", 0)\n",
    "            execution_time_sum += obj.get(\"execution_time (seconds/batch)\", 0)\n",
    "            throughput_bps_sum += obj.get(\"throughput_bps\", 0)\n",
    "\n",
    "        # Compute averages (avoid division by zero)\n",
    "        if count > 0:\n",
    "            avg_cpu_time = total_cpu_time_sum / count\n",
    "            avg_total_process_memory = total_process_memory_sum / count\n",
    "            avg_execution_time = execution_time_sum / count\n",
    "            avg_throughput_bps = throughput_bps_sum / count\n",
    "        else:\n",
    "            avg_cpu_time = avg_cpu_memory = avg_execution_time = avg_throughput_bps = 0\n",
    "\n",
    "        # Append summary for this file\n",
    "        summary_data.append({\n",
    "            \"filename\": os.path.basename(file_path),\n",
    "            \"avg_cpu_time\": avg_cpu_time,\n",
    "            \"avg_process_memory\": avg_total_process_memory,\n",
    "            \"avg_throughput_bps\": avg_throughput_bps,\n",
    "            \"record_count\": count\n",
    "        })\n",
    "\n",
    "# Create a DataFrame from the summary data\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "# Display the summary DataFrame\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0af06cf9-cbf9-48f7-ac3d-b06f4047d32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "json_files = glob.glob(os.path.join(current_dir, \"*.json\"))\n",
    "summary_data = []\n",
    "\n",
    "for file_path in json_files:\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "        # Initialize accumulators\n",
    "        total_cpu_time_sum = 0\n",
    "        total_process_memory_sum = 0\n",
    "        total_execution_time_sum = 0\n",
    "        total_throughput_bps_sum = 0\n",
    "\n",
    "        cpu_time_count = 0\n",
    "        process_memory_count = 0\n",
    "        execution_time_count = 0\n",
    "        throughput_bps_count = 0\n",
    "\n",
    "        for obj in data:\n",
    "            if \"total_cpu_time (seconds)\" in obj:\n",
    "                total_cpu_time_sum += obj[\"total_cpu_time (seconds)\"]\n",
    "                cpu_time_count += 1\n",
    "\n",
    "            if \"total_process_memory (MB)\" in obj:\n",
    "                total_process_memory_sum += obj[\"total_process_memory (MB)\"]\n",
    "                process_memory_count += 1\n",
    "\n",
    "            if \"execution_time (seconds/batch)\" in obj:\n",
    "                total_execution_time_sum += obj[\"execution_time (seconds/batch)\"]\n",
    "                execution_time_count += 1\n",
    "\n",
    "            if \"throughput_bps\" in obj:\n",
    "                total_throughput_bps_sum += obj[\"throughput_bps\"]\n",
    "                throughput_bps_count += 1\n",
    "\n",
    "        # Compute averages with separate counts\n",
    "        avg_cpu_time = total_cpu_time_sum / cpu_time_count if cpu_time_count > 0 else 0\n",
    "        avg_process_memory = total_process_memory_sum / process_memory_count if process_memory_count > 0 else 0\n",
    "        avg_execution_time = total_execution_time_sum / execution_time_count if execution_time_count > 0 else 0\n",
    "        avg_throughput_bps = total_throughput_bps_sum / throughput_bps_count if throughput_bps_count > 0 else 0\n",
    "\n",
    "        # Append summary for this file\n",
    "        summary_data.append({\n",
    "            \"filename\": os.path.basename(file_path),\n",
    "            \"avg_cpu_time\": avg_cpu_time,\n",
    "            \"avg_process_memory\": avg_process_memory,\n",
    "            \"avg_execution_time\": avg_execution_time,\n",
    "            \"avg_throughput_bps\": avg_throughput_bps,\n",
    "            \"cpu_time_count\": cpu_time_count,\n",
    "            \"process_memory_count\": process_memory_count})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79638888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                filename  avg_cpu_time  avg_process_memory  avg_throughput_bps\n",
      "0   100MB-1file-128.json     25.759324         2138.394531        3.258432e+07\n",
      "1   100MB-1file-256.json     20.604544         4361.402344        4.073616e+07\n",
      "2    100MB-1File-32.json    225.481818         2738.230469        3.722473e+06\n",
      "3   100MB-1file-512.json     21.341026         4517.609375        3.933035e+07\n",
      "4    100MB-1file-64.json     22.514245         1681.699219        3.728084e+07\n",
      "5     100MB-1GB-256.json     24.066571         4313.574219        3.535165e+07\n",
      "6      10MB-1GB-256.json      3.328900         1186.802790        2.607668e+07\n",
      "7      10MB-2GB-256.json      3.092958         1190.260110        2.886155e+07\n",
      "8      10MB-4GB-256.json      2.751213         1180.918400        3.301102e+07\n",
      "9      10MB-6GB-256.json      2.799368         1186.847075        3.219871e+07\n",
      "10     10MB-8GB-256.json      3.601294         1187.793519        2.493032e+07\n",
      "11    25MB-10GB-256.json      6.937246          190.800868        3.273441e+07\n",
      "12    25MB-12GB-256.json      6.271847         1729.517991        3.535525e+07\n",
      "13     25MB-1GB-256.json      6.465970         1720.837426        3.296535e+07\n",
      "14     25MB-2GB-256.json      6.886912          251.534948        3.211633e+07\n",
      "15     25MB-4GB-256.json      7.083553          252.553089        3.154276e+07\n",
      "16     25MB-6GB-256.json      6.664443          176.826941        3.373354e+07\n",
      "17     25MB-8GB-256.json      7.026916          237.251691        3.195521e+07\n",
      "18     50MB-1GB-256.json     11.523692         2564.420945        3.699282e+07\n",
      "19     75MB-1GB-256.json     18.530627         3434.301339        3.475404e+07\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a DataFrame from the summary data\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "# Display the summary DataFrame\n",
    "print(summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4516ef3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute charges: $0.0232, Request charges: $2.2e-05, Storage charges: $3.62e-05\n",
      "Total charges: $0.0232.\n"
     ]
    }
   ],
   "source": [
    "num_requests = 110\n",
    "duration = 7.1 # seconds\n",
    "memory_allocated = 1.78 # GB \n",
    "storage = 2 # GB\n",
    "\n",
    "compute_per_GB_s = 0.0000166667 # USD per GB/s\n",
    "GB_s = num_requests * duration * memory_allocated # compute power used in GB/s\n",
    "compute_charges = GB_s * compute_per_GB_s\n",
    "\n",
    "charges_per_request = 2e-7\n",
    "request_charges = num_requests * charges_per_request\n",
    "\n",
    "storage_per_GB_s = 3.09e-8 # USD per GB/s\n",
    "storage_charges = (storage - 0.5)* (num_requests * duration) *storage_per_GB_s if storage > 0.5 else 0\n",
    "\n",
    "total_charges = compute_charges + request_charges +  storage_charges\n",
    "\n",
    "print(f'Compute charges: ${compute_charges:.3g}, Request charges: ${request_charges:.3g}, Storage charges: ${storage_charges:.3g}')\n",
    "print(f'Total charges: ${total_charges:.3g}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "981916bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bucket': 'cosmicai-data',\n",
       " 'file_limit': '106',\n",
       " 'batch_size': 265,\n",
       " 'object_type': 'folder',\n",
       " 'S3_object_name': 'Anomaly Detection',\n",
       " 'script': '/tmp/Anomaly Detection/Inference/inference.py',\n",
       " 'result_path': 'scaling/result-partition-75MB/8GB/2',\n",
       " 'data_bucket': 'cosmicai-data',\n",
       " 'data_prefix': '75MB'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "  \"bucket\": \"cosmicai-data\",\n",
    "  \"file_limit\": \"106\",\n",
    "  \"batch_size\": 265,\n",
    "  \"object_type\": \"folder\",\n",
    "  \"S3_object_name\": \"Anomaly Detection\",\n",
    "  \"script\": \"/tmp/Anomaly Detection/Inference/inference.py\",\n",
    "  \"result_path\": \"scaling/result-partition-75MB/8GB/2\",\n",
    "  \"data_bucket\": \"cosmicai-data\",\n",
    "  \"data_prefix\": \"75MB\"\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
